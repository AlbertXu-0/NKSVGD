{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVGD gradient with deep kernel\n",
    "\n",
    "Define function `svgd_gradient(particals, score, h=-1)` for $\\phi(x)$ to update particals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining deep kernel\n",
    "'''\n",
    "def rbf_kernel(x, y, sigma=1.0):\n",
    "    K = torch.exp(-torch.cdist(x, y) ** 2 / (2 * sigma ** 2))\n",
    "    return K\n",
    "'''\n",
    "\n",
    "class KernelNN(nn.Module):\n",
    "    def __init__(self, in_dim=1, out_dim=1, hidden_dim=300, depth=1):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                nn.SiLU(hidden_dim),\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                nn.SiLU(hidden_dim),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.SiLU(hidden_dim),\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def svgd_grad_with_deepkernel(theta, score, model, h=-1):\n",
    "    particles = model(theta)\n",
    "    delta_x = particles.unsqueeze(0) - particles.unsqueeze(1)  # N x N x D\n",
    "    pairwise_dists = delta_x.pow(2.0).sum(-1)  # N x N\n",
    "    if h < 0: \n",
    "        # if h < 0, using median trick\n",
    "        h = torch.median(pairwise_dists)\n",
    "        h = torch.sqrt(0.5 * h / torch.log(torch.tensor(particles.shape[0]+1)))\n",
    "\n",
    "    # compute the rbf kernel\n",
    "    Kxy = torch.exp( - pairwise_dists / h**2 / 2)\n",
    "    dxkxy = torch.autograd.grad(Kxy, theta, grad_outputs=torch.rand_like(Kxy), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    repulsive_grad = dxkxy\n",
    "    attractive_grad = torch.mm(Kxy, score)\n",
    "    return (attractive_grad + repulsive_grad) / particles.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior(theta, X_train, y_train):\n",
    "    '''\n",
    "    Returns:\n",
    "        log density: 1-dim: sample_num\n",
    "    '''\n",
    "    W = theta  # sample_num x D\n",
    "    D = theta.shape[1]\n",
    "    Xs = X_train\n",
    "    ys = y_train  # batch x 1\n",
    "    z = torch.matmul(Xs, W.t())  # batch x sample_num\n",
    "    coff = -z * ys  # batch x sample_num\n",
    "    coff = torch.clamp(coff, min=-20, max=20)\n",
    "    log_p_D_given_w = -torch.log(1 + torch.exp(coff)).sum(0)  # sample_num\n",
    "    log_p_w_given_alpha = -0.5 * torch.sum(W * W, dim=1) + (D / 2) * math.log(1.)   # sample_num\n",
    "    log_posterior = log_p_D_given_w  + log_p_w_given_alpha\n",
    "    return log_posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.randn(100,10)\n",
    "y_train = torch.randint(1,(100,)) \n",
    "y_train[y_train == 0] = -1\n",
    "particles = torch.randn(100,10).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_posterior = log_posterior(particles, X_train= X_train, y_train= y_train)\n",
    "score = torch.autograd.grad(log_posterior.sum(), particles, grad_outputs=None, retain_graph=True, create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = X_train.shape[1]\n",
    "kernel_model = KernelNN(in_dim=target_dim, out_dim=target_dim)\n",
    "phi = svgd_grad_with_deepkernel(theta=particles, score=score, model=kernel_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1123, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi.sum(-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0112, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVGD with deep kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKSVGD:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, batch_size=100, alpha=1.0):\n",
    "        self.device = device\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "        self.batch_size = min(batch_size, self.X_train.shape[0])\n",
    "        self.alpha = 1.0 / alpha\n",
    "        self.N = X_train.shape[0]\n",
    "        self.permutation = torch.randperm(self.N)\n",
    "        self.iter = 0\n",
    "\n",
    "    def keep_grad(self, output, input, grad_outputs=None):\n",
    "        return torch.autograd.grad(output, input, grad_outputs=grad_outputs, retain_graph=True, create_graph=True)[0]\n",
    "    \n",
    "    def svgd_grad_with_deepkernel(self, theta, score, model, h=-1):\n",
    "        particles = model(theta)\n",
    "        delta_x = particles.unsqueeze(0) - particles.unsqueeze(1)  # N x N x D\n",
    "        pairwise_dists = delta_x.pow(2.0).sum(-1)  # N x N\n",
    "        if h < 0: \n",
    "            # if h < 0, using median trick\n",
    "            h = torch.median(pairwise_dists)\n",
    "            h = torch.sqrt(0.5 * h / torch.log(torch.tensor(particles.shape[0]+1)))\n",
    "        \n",
    "        # compute the rbf kernel\n",
    "        Kxy = torch.exp( - pairwise_dists / h**2 / 2)\n",
    "        dxkxy = torch.autograd.grad(Kxy, theta, grad_outputs=torch.ones_like(Kxy), retain_graph=True, create_graph=True)[0]\n",
    "        \n",
    "        repulsive_grad = dxkxy\n",
    "        attractive_grad = torch.mm(Kxy, score)\n",
    "        return (attractive_grad + repulsive_grad) / particles.shape[0]\n",
    "    \n",
    "    def log_posterior_f(self, theta, iter):\n",
    "        '''\n",
    "        Returns:\n",
    "            log density: 1-dim: sample_num\n",
    "        '''\n",
    "        W = theta  # sample_num x D\n",
    "        D = theta.shape[1]\n",
    "        batch = [i % self.N for i in range(iter * self.batch_size, (iter + 1) * self.batch_size)]\n",
    "        ridx = self.permutation[batch]\n",
    "        Xs = self.X_train[ridx, :]  # batch x D\n",
    "        ys = self.y_train[ridx]  # batch x 1\n",
    "        z = torch.matmul(Xs, W.t())  # batch x sample_num\n",
    "        coff = -z * ys  # batch x sample_num\n",
    "        coff = torch.clamp(coff, min=-20, max=20)\n",
    "        log_p_D_given_w = -torch.log(1 + torch.exp(coff)).sum(0)  # sample_num\n",
    "        log_p_w_given_alpha = -0.5 * self.alpha * torch.sum(W * W, dim=1) + (D / 2) * math.log(1.)   # sample_num\n",
    "        log_posterior = log_p_D_given_w * self.N / Xs.shape[0] + log_p_w_given_alpha\n",
    "        return log_posterior\n",
    "    \n",
    "    def evaluation(self, theta, X_test, y_test):\n",
    "        W = theta.cpu().detach() # N x D\n",
    "\n",
    "        #print('BLR weight: ', torch.mean(W, dim=0))\n",
    "        # print('BLR weight std: ', torch.std(W, dim=0))\n",
    "        z = torch.matmul(X_test, W.t())  # batch x smale_num\n",
    "        coff = -z * y_test  # batch x sample_num\n",
    "        prob = torch.mean(1. / (1 + torch.exp(coff)), dim=1)\n",
    "        acc = torch.mean((prob > .5).float())\n",
    "        llh = torch.mean(torch.log(prob))\n",
    "        return acc, llh\n",
    "\n",
    "    def update_particles_and_eval_iters(self, target_dim, hidden_dim, net_lr, initial_particles,\n",
    "                                        n_iter, step_size, reg_coefficient,auto_corr=0.9, \n",
    "                                        fudge_factor=1e-6):\n",
    "        cur_particles = initial_particles\n",
    "        historical_grad = 0\n",
    "        kernel_model = KernelNN(in_dim=target_dim, out_dim=target_dim, hidden_dim=hidden_dim)\n",
    "        optimizer = optim.Adam(kernel_model.parameters(), lr=net_lr, betas=(0.1, 0.1), amsgrad=True)\n",
    "        for i in tqdm(range(n_iter)): # progress bar\n",
    "            kernel_model.train()\n",
    "            optimizer.zero_grad()\n",
    "            cur_particles = cur_particles.requires_grad_()\n",
    "            log_p_target = self.log_posterior_f(cur_particles, i)\n",
    "            score_p = self.keep_grad(log_p_target.sum(), cur_particles)\n",
    "\n",
    "            f_x = svgd_grad_with_deepkernel(theta=particles, score=score_p, model=kernel_model)\n",
    "            stein_loss = f_x.sum(-1).mean()  # estimate of S(p, q)\n",
    "            loss = -1.0 * stein_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                kernel_model.eval()\n",
    "                phi = f_x\n",
    "                if i == 0:\n",
    "                    historical_grad = historical_grad + torch.multiply(phi, phi)\n",
    "                else:\n",
    "                    historical_grad = auto_corr * historical_grad + (1 - auto_corr) * torch.multiply(phi, phi)\n",
    "                adj_grad = torch.divide(phi, fudge_factor + torch.sqrt(historical_grad))\n",
    "                # adj_grad = phi\n",
    "                cur_particles = cur_particles + step_size * adj_grad\n",
    "                if (i + 1) % 100000 == 0:\n",
    "                    acc, ll = self.evaluation(cur_particles.detach(), self.X_test, self.y_test)\n",
    "                    print(f'Iter: {i + 1}, Acc:{acc:.4f}, LL:{ll:.4f}, phi:{f_x.mean():.4f}')\n",
    "        final_particles = cur_particles\n",
    "\n",
    "        return final_particles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DateSet Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import load_data_for_blr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self, num_particles, batch_size, num_trials):\n",
    "        self.num_particles = num_particles\n",
    "        self.batch_size = batch_size\n",
    "        self.num_trials = num_trials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preset\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num_particles', type=int, default=200, help=\"particles number\")\n",
    "parser.add_argument('--batch_size', type=int, default=100)\n",
    "parser.add_argument('--num_trials', type=int, default=20)\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed=123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training\n",
    "import time\n",
    "\n",
    "datasets = ['covertype', 'w8a', 'a9a', 'bioresponse']\n",
    "step_size = {'covertype': 0.008, 'w8a': 0.05, 'a9a': 0.03,  'bioresponse' : 0.003}\n",
    "hidden_dim = {'covertype': 100, 'w8a': 1000, 'a9a': 500, 'bioresponse' : 5000}\n",
    "depth = {'covertype': 2, 'w8a': 1, 'a9a': 1, 'bioresponse': 1}\n",
    "max_iters = {'covertype': 10000, 'w8a': 10000, 'a9a': 10000,  'bioresponse' : 10000}\n",
    "reg_coefficient = {'covertype': 5.0, 'w8a': 1.0, 'a9a': 1.0, 'bioresponse': 1.0}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_particles = 10||batch_size = 100||num_trials = 2\n"
     ]
    }
   ],
   "source": [
    "args = Arguments(10,100,2)\n",
    "print(f\"num_particles = {args.num_particles}||batch_size = {args.batch_size}||num_trials = {args.num_trials}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c274f3ff01d74fa9804ecc0d077d047c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x10 and 124x500)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9740\\1381438957.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mcur\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDKSVGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         final_particles = model.update_particles_and_eval_iters(\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mtarget_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0minitial_particles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9740\\4390777.py\u001b[0m in \u001b[0;36mupdate_particles_and_eval_iters\u001b[1;34m(self, target_dim, hidden_dim, net_lr, initial_particles, n_iter, step_size, reg_coefficient, auto_corr, fudge_factor)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mscore_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_p_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_particles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mf_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvgd_grad_with_deepkernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparticles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscore_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernel_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[0mstein_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# estimate of S(p, q)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstein_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9740\\3625491419.py\u001b[0m in \u001b[0;36msvgd_grad_with_deepkernel\u001b[1;34m(theta, score, model, h)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msvgd_grad_with_deepkernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mparticles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdelta_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparticles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mparticles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# N x N x D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpairwise_dists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# N x N\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9740\\4113953023.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x10 and 124x500)"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start = time.time()\n",
    "\n",
    "for dataset in datasets[2:3]:\n",
    "    acc = torch.zeros(args.num_trials)\n",
    "    ll = torch.zeros(args.num_trials)\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val = load_data_for_blr(dataset)\n",
    "\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "    D = X_train.shape[1]\n",
    "    for trial in range(args.num_trials):\n",
    "        cur = torch.rand(args.num_particles, D).to(device).requires_grad_()\n",
    "        model = DKSVGD(X_train, y_train, X_test, y_test, batch_size=args.batch_size)\n",
    "        final_particles = model.update_particles_and_eval_iters(\n",
    "            target_dim=D, hidden_dim=hidden_dim[dataset], net_lr=1e-4,\n",
    "            initial_particles=cur, n_iter=max_iters[dataset],\n",
    "            step_size=step_size[dataset], reg_coefficient=reg_coefficient[dataset])\n",
    "        acc[trial], ll[trial] = model.evaluation(final_particles, X_test, y_test)\n",
    "\n",
    "    print(f'Dataset: {dataset}, '\n",
    "            f'Acc mean: {acc.mean().item():.6f}, Acc std: {acc.std().item():.6f}, '\n",
    "            f'Likelihood mean: {ll.mean().item():.6f}, Likelihood std: {ll.std().item():.6f}')\n",
    "end = time.time()\n",
    "print(f'Run time: {end - start:.4f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
