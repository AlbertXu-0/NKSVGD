{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVGD gradient with deep kernel\n",
    "\n",
    "Define function `svgd_gradient(particals, score, h=-1)` for $\\phi(x)$ to update particals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining deep kernel\n",
    "'''\n",
    "def rbf_kernel(x, y, sigma=1.0):\n",
    "    K = torch.exp(-torch.cdist(x, y) ** 2 / (2 * sigma ** 2))\n",
    "    return K\n",
    "'''\n",
    "\n",
    "class KernelLayer(nn.Module):\n",
    "    def __init__(self, input_dim, kernel):\n",
    "        super(KernelLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.kernel = kernel\n",
    "        \n",
    "    def forward(self, x):\n",
    "        k = self.kernel(x, x)\n",
    "        return k\n",
    "\n",
    "class KernelNN(nn.Module):\n",
    "    def __init__(self, kernel, in_dim=1, out_dim=1, hidden_dim=300, depth=1):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                Swish(hidden_dim),\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                KernelLayer(out_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                Swish(hidden_dim),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                Swish(hidden_dim),\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                KernelLayer(out_dim),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def svgd_kernel(x, h=-1):\n",
    "    theta = \n",
    "    sq_dist = pdist(theta)\n",
    "    pairwise_dists = squareform(sq_dist) ** 2\n",
    "    pairwise_dists = torch.tensor(pairwise_dists)\n",
    "    if h < 0: \n",
    "        # if h < 0, using median trick\n",
    "        h = torch.median(pairwise_dists)\n",
    "        h = torch.sqrt(0.5 * h / torch.log(torch.tensor(theta.shape[0]+1)))\n",
    "\n",
    "    # compute the rbf kernel\n",
    "    Kxy = torch.exp( - pairwise_dists / h**2 / 2)\n",
    "    Kxy = Kxy.double()\n",
    "    dxkxy = - torch.matmul(Kxy, theta)\n",
    "    sumkxy = torch.sum(Kxy, axis=1)\n",
    "    for i in range(theta.shape[1]):\n",
    "        dxkxy[:, i] = dxkxy[:, i] + torch.mul(theta[:, i], sumkxy)\n",
    "    dxkxy = dxkxy / (h**2)\n",
    "\n",
    "    return Kxy, dxkxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svgd_gradient(particles, score, h=-1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        particles: N x D\n",
    "        score: gradient of log p(x):  N x D\n",
    "        h: -1\n",
    "    Returns:\n",
    "        svgd update gradient, phi(*) in the paper\n",
    "    \"\"\"\n",
    "\n",
    "    delta_x = particles.unsqueeze(0) - particles.unsqueeze(1)  # N x N x D\n",
    "    pairwise_dists = delta_x.pow(2.0).sum(-1)  # N x N\n",
    "\n",
    "    if h < 0:  # if h < 0, using median trick\n",
    "        h = torch.median(pairwise_dists)\n",
    "        h = torch.sqrt(0.5 * h / math.log(particles.shape[0] + 1))  # in fact is sqrt(1 / (2h))\n",
    "\n",
    "    Kxy = torch.exp(-pairwise_dists / h ** 2 / 2)  # NxN , rbf kernel matrix\n",
    "\n",
    "    dxkxy1 = -torch.matmul(Kxy, particles)\n",
    "    sumkxy = torch.sum(Kxy, dim=1, keepdim=True)  # N x 1\n",
    "    dxkxy2 = sumkxy * particles\n",
    "    dxkxy = (dxkxy1 + dxkxy2) / (h ** 2)\n",
    "    repulsive_grad = dxkxy\n",
    "    attractive_grad = torch.mm(Kxy, score)\n",
    "    return (attractive_grad + repulsive_grad) / particles.shape[0]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swish neural & MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super().__init__()\n",
    "        if dim > 0:\n",
    "            self.beta = nn.Parameter(torch.ones((dim,)))\n",
    "        else:\n",
    "            self.beta = torch.ones((1,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) == 2:\n",
    "            return x * torch.sigmoid(self.beta[None, :] * x)\n",
    "        else:\n",
    "            return x * torch.sigmoid(self.beta[None, :, None, None] * x)\n",
    "        \n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self,data_dim):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 2))\n",
    "\n",
    "class KernelLayer(nn.Module):\n",
    "    def __init__(self, input_dim, kernel):\n",
    "        super(KernelLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.kernel = kernel\n",
    "        \n",
    "    def forward(self, x):\n",
    "        k = self.kernel(x, x)\n",
    "        return k\n",
    "\n",
    "class KernelNN(nn.Module):\n",
    "    def __init__(self, kernel, in_dim=1, out_dim=1, hidden_dim=300, depth=1):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                Swish(hidden_dim),\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                KernelLayer(out_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                Swish(hidden_dim),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                Swish(hidden_dim),\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                KernelLayer(out_dim),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.randn(100,10).requires_grad_()\n",
    "data_dim = train_x.size(-1)\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor(data_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_projected=feature_extractor(train_x)\n",
    "x_projected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x, y, sigma=1.0):\n",
    "    K = torch.exp(-torch.cdist(x, y) ** 2 / (2 * sigma ** 2))\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_grad(output, input, grad_outputs=None):\n",
    "    return torch.autograd.grad(output, input, grad_outputs=grad_outputs, retain_graph=True, create_graph=True)[0]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_projected_vice = x_projected\n",
    "kernel_matrix = rbf_kernel(x_projected,x_projected_vice.detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps=torch.rand_like(kernel_matrix)\n",
    "dkxy_dx = keep_grad(output=kernel_matrix,input=train_x,grad_outputs=eps)\n",
    "dkxy_dx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x:2*x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Neural SVGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNeuralSVGD:\n",
    "\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def keep_grad(self, output, input, grad_outputs=None):\n",
    "        return torch.autograd.grad(output, input, grad_outputs=grad_outputs, retain_graph=True, create_graph=True)[0]\n",
    "    \n",
    "    def rbf_kernel(x, sigma=-1.0):\n",
    "        K = torch.exp(-torch.cdist(x, y) ** 2 / (2 * sigma ** 2))\n",
    "        return K\n",
    "\n",
    "    def approx_jacobian_trace(self, fx, x):\n",
    "        eps = torch.randn_like(fx)\n",
    "        eps_dfdx = self.keep_grad(fx, x, grad_outputs=eps)\n",
    "        tr_dfdx = (eps_dfdx * eps).sum(-1)\n",
    "        return tr_dfdx\n",
    "\n",
    "    def exact_jacobian_trace(self, fx, x):\n",
    "        vals = []\n",
    "        for i in range(x.size(1)):\n",
    "            fxi = fx[:, i]\n",
    "            dfxi_dxi = self.keep_grad(fxi.sum(), x)[:, i][:, None]\n",
    "            vals.append(dfxi_dxi)\n",
    "        vals = torch.cat(vals, dim=1)\n",
    "        return vals.sum(dim=1)\n",
    "\n",
    "    def update_particles_base(self, log_p_target, data_dim, hidden_dim, net_lr, net_update_num, initial_particles,\n",
    "                              n_iter, step_size, reg_coefficient, init_net_per_iter=1, jacobian_trace='approx'):\n",
    "        cur_particles = initial_particles\n",
    "        stein_discrepancy = []\n",
    "        trans_net = LargeFeatureExtractor(data_dim=data_dim).to(self.device)\n",
    "        kernel_mat = lambda particles: self.rbf_kernel(particles, particles)\n",
    "        f_net = lambda \n",
    "        optimizer = optim.Adam(f_net.parameters(), lr=net_lr)\n",
    "        for i in trange(n_iter):\n",
    "            if (i + 1) % init_net_per_iter == 0:\n",
    "                f_net = MLP(in_dim=data_dim, out_dim=data_dim, hidden_dim=hidden_dim, activation=activation).to(self.device)\n",
    "                optimizer = optim.SGD(f_net.parameters(), lr=net_lr)\n",
    "            f_net.train()\n",
    "            for j in range(net_update_num):\n",
    "                optimizer.zero_grad()\n",
    "                cur_particles = cur_particles.requires_grad_()\n",
    "                f_x = f_net(cur_particles)\n",
    "                if jacobian_trace == 'exact':\n",
    "                    tr_grad_f = self.exact_jacobian_trace(f_x, cur_particles)\n",
    "                else:\n",
    "                    tr_grad_f = self.approx_jacobian_trace(f_x, cur_particles)\n",
    "                score_p = self.keep_grad(log_p_target(cur_particles).sum(), cur_particles)\n",
    "                scorep_fx = (score_p * f_x).sum(-1)  # compute (dlogp(x)/dx)^T * f(x)\n",
    "                stein_loss = (scorep_fx + tr_grad_f).mean()  # estimate of S(p, q)\n",
    "                l2_penalty = (f_x * f_x).sum(1).mean() * reg_coefficient\n",
    "                loss = -1.0 * stein_loss + l2_penalty\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if j == net_update_num - 1:\n",
    "                    stein_discrepancy.append(stein_loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                f_net.eval()\n",
    "                cur_particles = cur_particles + step_size * f_net(cur_particles)\n",
    "        final_particles = cur_particles\n",
    "\n",
    "        return final_particles, stein_discrepancy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesianLR model with svgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "class BayesianLR(BaseNeuralSVGD):\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, batch_size=100, alpha=1.0):\n",
    "        super().__init__(device=device)\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "        self.batch_size = min(batch_size, self.X_train.shape[0])\n",
    "        self.alpha = 1.0 / alpha\n",
    "        self.N = X_train.shape[0]\n",
    "        self.permutation = torch.randperm(self.N)\n",
    "        self.iter = 0\n",
    "\n",
    "    def log_posterior(self, theta, iter):\n",
    "        '''\n",
    "        Returns:\n",
    "            log density: 1-dim: sample_num\n",
    "        '''\n",
    "        W = theta  # sample_num x D\n",
    "        D = theta.shape[1]\n",
    "        batch = [i % self.N for i in range(iter * self.batch_size, (iter + 1) * self.batch_size)]\n",
    "        ridx = self.permutation[batch]\n",
    "        Xs = self.X_train[ridx, :]  # batch x D\n",
    "        ys = self.y_train[ridx]  # batch x 1\n",
    "        z = torch.matmul(Xs, W.t())  # batch x sample_num\n",
    "        coff = -z * ys  # batch x sample_num\n",
    "        coff = torch.clamp(coff, min=-20, max=20)\n",
    "\n",
    "        log_p_D_given_w = -torch.log(1 + torch.exp(coff)).sum(0)  # sample_num\n",
    "        log_p_w_given_alpha = -0.5 * self.alpha * torch.sum(W * W, dim=1) + (D / 2) * math.log(self.alpha)   # sample_num\n",
    "        log_posterior = log_p_D_given_w * self.N / Xs.shape[0] + log_p_w_given_alpha\n",
    "        return log_posterior\n",
    "\n",
    "    def evaluation(self, theta, X_test, y_test):\n",
    "        W = theta.cpu().detach() # N x D\n",
    "\n",
    "        #print('BLR weight: ', torch.mean(W, dim=0))\n",
    "        # print('BLR weight std: ', torch.std(W, dim=0))\n",
    "        z = torch.matmul(X_test, W.t())  # batch x smale_num\n",
    "        coff = -z * y_test  # batch x sample_num\n",
    "        prob = torch.mean(1. / (1 + torch.exp(coff)), dim=1)\n",
    "        acc = torch.mean((prob > .5).float())\n",
    "        llh = torch.mean(torch.log(prob))\n",
    "        return acc, llh\n",
    "\n",
    "    def update_particles_and_eval_iters(self, target_dim, hidden_dim, net_lr, initial_particles,\n",
    "                                        n_iter, step_size, reg_coefficient, jacobian_trace='approx',\n",
    "                                        auto_corr=0.9, fudge_factor=1e-6):\n",
    "        cur_particles = initial_particles\n",
    "        historical_grad = 0\n",
    "        kernel = MLP(in_dim=target_dim, out_dim=target_dim, hidden_dim=hidden_dim).to(self.device)\n",
    "        f_net = MLP(in_dim=target_dim, out_dim=target_dim, hidden_dim=hidden_dim).to(self.device)\n",
    "        optimizer = optim.Adam(f_net.parameters(), lr=net_lr, betas=(0.1, 0.1), amsgrad=True)\n",
    "        for i in tqdm(range(n_iter)): # progress bar\n",
    "            f_net.train()\n",
    "            optimizer.zero_grad()\n",
    "            cur_particles = cur_particles.detach().requires_grad_()\n",
    "            f_x = f_net(cur_particles)\n",
    "            if jacobian_trace == 'exact':\n",
    "                tr_grad_f = self.exact_jacobian_trace(f_x, cur_particles)\n",
    "            else:\n",
    "                tr_grad_f = self.approx_jacobian_trace(f_x, cur_particles)\n",
    "            score_p = self.keep_grad(self.log_posterior(cur_particles, i).sum(), cur_particles) # B x D\n",
    "            scorep_fx = (score_p * f_x).sum(-1) # compute (dlogp(x)/dx)^T * f(x)\n",
    "            stein_loss = (scorep_fx + tr_grad_f).mean()  # estimate of S(p, q)\n",
    "            l2_penalty = (f_x * f_x).sum(-1).mean() * reg_coefficient\n",
    "            loss = -1.0 * stein_loss + l2_penalty\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(f_net.parameters(), 1e8)\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                f_net.eval()\n",
    "                phi = f_net(cur_particles)\n",
    "                if i == 0:\n",
    "                    historical_grad = historical_grad + torch.multiply(phi, phi)\n",
    "                else:\n",
    "                    historical_grad = auto_corr * historical_grad + (1 - auto_corr) * torch.multiply(phi, phi)\n",
    "                adj_grad = torch.divide(phi, fudge_factor + torch.sqrt(historical_grad))\n",
    "                # adj_grad = phi\n",
    "                cur_particles = cur_particles + step_size * adj_grad\n",
    "                if (i + 1) % 100000 == 0:\n",
    "                    acc, ll = self.evaluation(cur_particles.detach(), self.X_test, self.y_test)\n",
    "                    print(f'Iter: {i + 1}, Acc:{acc:.4f}, LL:{ll:.4f}, phi:{f_x.mean():.4f}')\n",
    "        final_particles = cur_particles\n",
    "\n",
    "        return final_particles\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DateSet Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import load_data_for_blr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self, num_particles, batch_size, num_trials):\n",
    "        self.num_particles = num_particles\n",
    "        self.batch_size = batch_size\n",
    "        self.num_trials = num_trials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preset\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num_particles', type=int, default=200, help=\"particles number\")\n",
    "parser.add_argument('--batch_size', type=int, default=100)\n",
    "parser.add_argument('--num_trials', type=int, default=20)\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed=123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training\n",
    "import time\n",
    "\n",
    "datasets = ['covertype', 'w8a', 'a9a', 'bioresponse']\n",
    "step_size = {'covertype': 0.008, 'w8a': 0.05, 'a9a': 0.03,  'bioresponse' : 0.003}\n",
    "hidden_dim = {'covertype': 100, 'w8a': 1000, 'a9a': 500, 'bioresponse' : 5000}\n",
    "depth = {'covertype': 2, 'w8a': 1, 'a9a': 1, 'bioresponse': 1}\n",
    "max_iters = {'covertype': 10000, 'w8a': 10000, 'a9a': 10000,  'bioresponse' : 10000}\n",
    "reg_coefficient = {'covertype': 5.0, 'w8a': 1.0, 'a9a': 1.0, 'bioresponse': 1.0}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_particles = 10||batch_size = 100||num_trials = 2\n"
     ]
    }
   ],
   "source": [
    "args = Arguments(10,100,2)\n",
    "print(f\"num_particles = {args.num_particles}||batch_size = {args.batch_size}||num_trials = {args.num_trials}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel as a MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
